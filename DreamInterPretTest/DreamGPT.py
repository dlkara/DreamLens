import os
import time
from dotenv import load_dotenv
import faiss
import json
import numpy as np
import openai
import streamlit as st


# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(r'../env.txt')
print(f'\tOPENAI_API_KEY={os.getenv("OPENAI_API_KEY")[:20]}...')


@st.cache_resource
def load_data_and_build_index():
    """
    Faiss ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„°, ê·¸ë¦¬ê³  ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¡œë“œí•˜ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ì˜ ê²°ê³¼ëŠ” ìºì‹œë˜ì–´ ë°˜ë³µ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    os.makedirs("./embeddings", exist_ok=True)
    os.makedirs("./data", exist_ok=True)
    index_path = r"./embeddings/dream.index"
    metadata_path = r"./data/meta_dream.json"
    original_data_path = r'./data/dream.json'

    categories = {"ëŒ€ë¶„ë¥˜": set(), "ì†Œë¶„ë¥˜": set()}

    try:
        with open(original_data_path, 'r', encoding='utf-8') as f:
            total_data = json.load(f)
        for main_cat, sub_cats in total_data.items():
            categories["ëŒ€ë¶„ë¥˜"].add(main_cat.replace('"', ""))
            for sub_cat in sub_cats.keys():
                categories["ì†Œë¶„ë¥˜"].add(sub_cat)
        categories["ëŒ€ë¶„ë¥˜"] = sorted(list(categories["ëŒ€ë¶„ë¥˜"]))
        categories["ì†Œë¶„ë¥˜"] = sorted(list(categories["ì†Œë¶„ë¥˜"]))

    except FileNotFoundError:
        st.error(f"ë¶„ë¥˜ë¥¼ ìœ„í•´ ì›ë³¸ ë°ì´í„° íŒŒì¼ì¸ '{original_data_path}' íŒŒì¼ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    if os.path.exists(index_path) and os.path.exists(metadata_path):
        print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        faiss_index = faiss.read_index(index_path)
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        st.info("ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        progress_bar = st.progress(0, text="ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        documents, metadata = [], []
        for main_cat, sub_cats in total_data.items():
            for sub_cat, dreams in sub_cats.items():
                if isinstance(dreams, list):
                    for dream_item in dreams:
                        dream_text = dream_item.get('ê¿ˆ', '').replace('"', '').strip()
                        interp_text = dream_item.get('í•´ëª½', '').strip()
                        main_cat_text = main_cat.replace('"', '').strip()
                        if dream_text and interp_text:
                            full_text = f"ëŒ€ë¶„ë¥˜: {main_cat_text}\nì†Œë¶„ë¥˜: {sub_cat}\nê¿ˆ: {dream_text}\ní•´ëª½: {interp_text}"
                            documents.append(full_text)
                            metadata.append({"ëŒ€ë¶„ë¥˜": main_cat, "ì†Œë¶„ë¥˜": sub_cat, "ê¿ˆ": dream_text, "í•´ëª½": interp_text})

        progress_bar.progress(20, text=f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.")

        batch_size = 1000
        embedding_vectors = []
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            response = openai.embeddings.create(input=batch, model="text-embedding-3-small")
            embedding_vectors.extend([data.embedding for data in response.data])
            progress_percent = 20 + int(((i + len(batch)) / len(documents)) * 60)
            progress_bar.progress(progress_percent, text=f"{i + len(batch)}/{len(documents)} ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ...")
            time.sleep(1)

        embedding_vectors = np.array(embedding_vectors, dtype='float32')
        d = embedding_vectors.shape[1]
        progress_bar.progress(80, text="Faiss ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        faiss_index = faiss.IndexFlatL2(d)
        faiss_index.add(embedding_vectors)
        faiss.write_index(faiss_index, index_path)
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        progress_bar.progress(100, text="ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        time.sleep(2)
        progress_bar.empty()
        st.success("âœ… Faiss ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    return faiss_index, metadata, categories


@st.cache_data(show_spinner=False)
def get_embedding(text, model="text-embedding-3-small"):
    print(f"'{text[:20]}...'ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. (API í˜¸ì¶œ)")
    response = openai.embeddings.create(input=[text], model=model)
    return np.array([response.data[0].embedding], dtype='float32')


@st.cache_data(show_spinner=False)
def generate_llm_response(user_dream, retrieved_data_json, categories):
    print(f"'{user_dream[:20]}...'ì— ëŒ€í•œ ë¶„ë¥˜, ë‹µë³€, ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. (API í˜¸ì¶œ)")
    retrieved_data = json.loads(retrieved_data_json)
    reference_texts = []
    for i, item in enumerate(retrieved_data):
        clean_dream = item.get('ê¿ˆ', '').strip()
        clean_interp = item.get('í•´ëª½', '').strip()
        reference_texts.append(f"{i + 1}. ê¿ˆ: {clean_dream}\n   í•´ëª½: {clean_interp}")

    reference_section = "\n\n".join(reference_texts)

    # âœ¨ [ìˆ˜ì •] AIê°€ ì œëª©ì´ë‚˜ ë²ˆí˜¸ë¥¼ ë¶™ì´ì§€ ì•Šê³  ë‚´ìš©ë§Œ ì¶œë ¥í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ì„ ìˆ˜ì •
    prompt = f"""
ë‹¹ì‹ ì€ ê¿ˆ í•´ëª½ê³¼ ë¶„ë¥˜ì— ë§¤ìš° ëŠ¥ìˆ™í•œ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê¿ˆì„ ë¶„ì„í•˜ê³ , ë„¤ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---
[ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´]
- ê°€ëŠ¥í•œ ëŒ€ë¶„ë¥˜: {categories['ëŒ€ë¶„ë¥˜']}
- ê°€ëŠ¥í•œ ì†Œë¶„ë¥˜: {categories['ì†Œë¶„ë¥˜']}

[í•´ëª½ ì°¸ê³  ì •ë³´]
- ìœ ì‚¬í•œ ê¿ˆ ë°ì´í„°ë² ì´ìŠ¤:
{reference_section}
---
[ì‚¬ìš©ì ê¿ˆ ì´ì•¼ê¸°]:
{user_dream}
---
[ì‘ì—… ì§€ì¹¨ ë° ì¶œë ¥ í˜•ì‹]:
ë‹¹ì‹ ì€ ë°˜ë“œì‹œ ì•„ë˜ 4ê°œì˜ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ê° ë¶€ë¶„ì€ ì§€ì •ëœ êµ¬ë¶„ìë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
**ì ˆëŒ€ë¡œ, ì ˆëŒ€ë¡œ ê° ë¶€ë¶„ì— ì œëª©ì´ë‚˜ ë²ˆí˜¸(ì˜ˆ: "2. ìƒì„¸ í•´ëª½:")ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ë‚´ìš©ë§Œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.**

- **ì²« ë²ˆì§¸ ë¶€ë¶„**: `[ë¶„ë¥˜ì‹œì‘]`ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. [ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´]ë¥¼ ì°¸ê³ í•˜ì—¬ "ëŒ€ë¶„ë¥˜: [ì„ íƒ]\nì†Œë¶„ë¥˜: [ì„ íƒ]" í˜•ì‹ìœ¼ë¡œ ê¿ˆì„ ë¶„ë¥˜í•˜ì„¸ìš”. ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ "ëŒ€ë¶„ë¥˜: í•´ë‹¹ ì—†ìŒ\nì†Œë¶„ë¥˜: í•´ë‹¹ ì—†ìŒ" ì´ë¼ê³  ì ìœ¼ì„¸ìš”.

- **ë‘ ë²ˆì§¸ ë¶€ë¶„**: `[í•´ëª½ì‹œì‘]`ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. "ì‚¬ìš©ìë‹˜ì˜ ê¿ˆì„ ìì„¸íˆ ì‚´í´ë³´ë‹ˆ..." ì™€ ê°™ì´ ì¹œê·¼í•œ ë§íˆ¬ë¡œ ì‹œì‘í•˜ì—¬ ìƒì„¸í•œ í•´ëª½ê³¼ ë”°ëœ»í•œ ì¡°ì–¸ì„ ì‘ì„±í•˜ì„¸ìš”.

- **ì„¸ ë²ˆì§¸ ë¶€ë¶„**: `[í‚¤ì›Œë“œì¶”ì¶œ]`ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ê¿ˆì˜ ì˜ë¯¸ë¥¼ ì••ì¶•í•˜ëŠ” í•µì‹¬ ëª…ì‚¬ í‚¤ì›Œë“œ 3~4ê°œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì„œ ë‚˜ì—´í•˜ì„¸ìš”.

- **ë„¤ ë²ˆì§¸ ë¶€ë¶„**: `[ìš”ì•½ì‹œì‘]`ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ìƒì„¸ í•´ëª½ì˜ ë‚´ìš©ì„ ì„¸ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. ê° ë¬¸ì¥ì€ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ë¡œ ì‹œì‘í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê¿ˆì„ ë¶„ì„í•˜ê³ , ë¶„ë¥˜í•˜ê³ , í•´ëª½í•˜ê³ , ìš”ì•½í•˜ëŠ” ë‹¤ì¬ë‹¤ëŠ¥í•œ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# --- ì•± ì‹¤í–‰ ---
st.title("ğŸ”® AI ê¿ˆ í•´ëª½ ì—°êµ¬ì†Œ")
st.write("---")

faiss_index, metadata, categories = load_data_and_build_index()

user_dream = st.text_area("ì–´ë–¤ ê¿ˆì„ ê¾¸ì…¨ë‚˜ìš”?", height=150, placeholder="ëŒì•„ê°€ì‹  í• ë¨¸ë‹ˆê°€ í™˜í•˜ê²Œ ì›ƒìœ¼ë©° ëˆì„ ì£¼ì…¨ì–´ìš”.")

if st.button("í•´ëª½ ê²°ê³¼ ë³´ê¸°"):
    if not user_dream:
        st.warning("ê¿ˆ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("AIê°€ ê¿ˆì„ ë¶„ì„í•˜ê³  í•´ëª½ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            query_vector = get_embedding(user_dream)
            k = 5
            distances, indices = faiss_index.search(query_vector, k)
            retrieved_results = [metadata[i] for i in indices[0]]
            retrieved_results_json = json.dumps(retrieved_results, ensure_ascii=False)

            raw_answer = generate_llm_response(user_dream, retrieved_results_json, categories)

            try:
                # ë” ì•ˆì •ì ì¸ íŒŒì‹±ì„ ìœ„í•´ ìˆœì„œëŒ€ë¡œ ë¶„ë¦¬
                _, classification_part = raw_answer.split("[ë¶„ë¥˜ì‹œì‘]", 1)
                classification_part, interpretation_part = classification_part.split("[í•´ëª½ì‹œì‘]", 1)
                interpretation_part, keywords_part = interpretation_part.split("[í‚¤ì›Œë“œì¶”ì¶œ]", 1)
                keywords_part, summary_part = keywords_part.split("[ìš”ì•½ì‹œì‘]", 1)

            except ValueError:
                # LLMì´ í˜•ì‹ì— ë§ì§€ ì•Šê²Œ ë‹µë³€í–ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
                classification_part = "ë¶„ë¥˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                interpretation_part = raw_answer
                keywords_part = "í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                summary_part = "ìš”ì•½ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        # ë¶„ë¦¬ëœ ê° ë¶€ë¶„ì„ í™”ë©´ì— í‘œì‹œ
        st.subheader("ğŸ“Š AIê°€ ë¶„ì„í•œ ë‚˜ì˜ ê¿ˆ ì¢…ë¥˜")
        st.success(classification_part.strip())

        st.subheader("ğŸŒ™ AIê°€ ë“¤ë ¤ì£¼ëŠ” ë‚˜ì˜ ê¿ˆ ì´ì•¼ê¸°")
        st.markdown(interpretation_part.strip())

        st.subheader("ğŸ”‘ ê¿ˆì˜ í•µì‹¬ í‚¤ì›Œë“œ")
        st.info(keywords_part.strip())

        st.write("---")
        st.subheader("âœ¨ ì„¸ ì¤„ ìš”ì•½")

        summary_part_cleaned = summary_part.strip().replace("undefined", "").strip()
        st.info(summary_part_cleaned)