import os
import time
from dotenv import load_dotenv
import faiss
import json
import numpy as np
import openai

load_dotenv(r'../env/langchain_env.txt')
print(f'\tOPENAI_API_KEY={os.getenv("OPENAI_API_KEY")[:20]}...') # OPENAI_API_KEY í•„ìš”!
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st

from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_openai.chat_models.base import ChatOpenAI
from langchain_core.runnables.base import RunnableLambda
from langchain_core.runnables.passthrough import RunnablePassthrough
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain_openai.embeddings.base import OpenAIEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_text_splitters.character import CharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.callbacks.base import BaseCallbackHandler

# ìƒì„±í•  íŒŒì¼ë“¤ì˜ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ë³€ìˆ˜ë¡œ ì§€ì •
# í´ë”ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒì„±í•´ì£¼ëŠ” ë¡œì§ ì¶”ê°€
# --- 1. ë°ì´í„° ë° ì¸ë±ìŠ¤ ë¡œë”©/ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---

# --- 1. ë°ì´í„° ë° ì¸ë±ìŠ¤ ë¡œë”©/ìƒì„± (ìºì‹± ì ìš©) ---

# â­ï¸ [ìˆ˜ì •] @st.cache_resource ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ í•¨ìˆ˜ ì „ì²´ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.
# ì´ í•¨ìˆ˜ëŠ” ì•± ì„¸ì…˜ ë™ì•ˆ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
@st.cache_resource
def load_or_create_index_and_metadata():
    """
    Faiss ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ì˜ ê²°ê³¼ëŠ” ìºì‹œë˜ì–´ ë°˜ë³µ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    os.makedirs("./embeddings", exist_ok=True)
    os.makedirs("./data", exist_ok=True)
    index_path = r"./embeddings/dream.index"
    metadata_path = r"./data/meta_dream.json"
    original_data_path = r'./data/dream.json'

    if os.path.exists(index_path) and os.path.exists(metadata_path):
        print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        faiss_index = faiss.read_index(index_path)
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        st.info("ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        progress_bar = st.progress(0, text="ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        try:
            with open(original_data_path, 'r', encoding='utf-8') as f:
                total_data = json.load(f)
        except FileNotFoundError:
            st.error(f"'{original_data_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        documents, metadata = [], []
        for main_cat, sub_cats in total_data.items():
            for sub_cat, dreams in sub_cats.items():
                if isinstance(dreams, list):
                    for dream_item in dreams:
                        dream_text = dream_item.get('ê¿ˆ', '').replace('"', '').strip()
                        interp_text = dream_item.get('í•´ëª½', '').strip()
                        main_cat_text = main_cat.replace('"', '').strip()
                        if dream_text and interp_text:
                            full_text = f"ëŒ€ë¶„ë¥˜: {main_cat_text}\nì†Œë¶„ë¥˜: {sub_cat}\nê¿ˆ: {dream_text}\ní•´ëª½: {interp_text}"
                            documents.append(full_text)
                            metadata.append({"ëŒ€ë¶„ë¥˜": main_cat, "ì†Œë¶„ë¥˜": sub_cat, "ê¿ˆ": dream_text, "í•´ëª½": interp_text})

        progress_bar.progress(20, text=f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.")

        batch_size = 1000
        embedding_vectors = []
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            response = openai.embeddings.create(input=batch, model="text-embedding-3-small")
            embedding_vectors.extend([data.embedding for data in response.data])
            progress_percent = 20 + int(((i + len(batch)) / len(documents)) * 60)
            progress_bar.progress(progress_percent, text=f"{i + len(batch)}/{len(texts)} ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ...")
            time.sleep(1)

        embedding_vectors = np.array(embedding_vectors, dtype='float32')
        d = embedding_vectors.shape[1]
        progress_bar.progress(80, text="Faiss ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        faiss_index = faiss.IndexFlatL2(d)
        faiss_index.add(embedding_vectors)
        faiss.write_index(faiss_index, index_path)
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        progress_bar.progress(100, text="ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        time.sleep(2)
        progress_bar.empty()
        st.success("âœ… Faiss ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    return faiss_index, metadata


# --- 2. Streamlit UI êµ¬ì„± (LLM ì—°ë™ ë¶€ë¶„ ìˆ˜ì •) ---

# â­ï¸ [ìˆ˜ì •] @st.cache_data ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ API í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.
@st.cache_data
def get_embedding(text, model="text-embedding-3-small"):
    print(f"'{text[:20]}...'ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. (API í˜¸ì¶œ)")
    response = openai.embeddings.create(input=[text], model=model)
    return np.array([response.data[0].embedding], dtype='float32')


@st.cache_data
def generate_llm_response(user_dream, retrieved_data_json):
    """
    LLM ë‹µë³€ ìƒì„± í•¨ìˆ˜ë„ ìºì‹±í•˜ì—¬ ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ API í˜¸ì¶œì„ ë°©ì§€í•©ë‹ˆë‹¤.
    ì£¼ì˜: retrieved_dataëŠ” ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ë¼ ì§ì ‘ ìºì‹±ì´ ì–´ë ¤ìš°ë¯€ë¡œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"'{user_dream[:20]}...'ì— ëŒ€í•œ LLM ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (API í˜¸ì¶œ)")
    retrieved_data = json.loads(retrieved_data_json)
    reference_texts = []
    for i, item in enumerate(retrieved_data):
        clean_dream = item.get('ê¿ˆ', '').strip()
        clean_interp = item.get('í•´ëª½', '').strip()
        reference_texts.append(f"{i + 1}. ê¿ˆ: {clean_dream}\n   í•´ëª½: {clean_interp}")

    reference_section = "\n\n".join(reference_texts)
    prompt = f"""
ë‹¹ì‹ ì€ ìˆ˜ì‹­ ë…„ ê²½ë ¥ì˜ ë§¤ìš° í†µì°°ë ¥ ìˆê³  ë‹¤ì •í•œ í•´ëª½ ì „ë¬¸ê°€, 'ê¿ˆì˜ ì¡°ê° ì—°êµ¬ì†Œ'ì˜ ì—°êµ¬ì›ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ê¿ˆ ì´ì•¼ê¸°ë¥¼ ë“£ê³ , ì•„ë˜ ì œê³µëœ [í•´ëª½ ë°ì´í„°ë² ì´ìŠ¤]ì—ì„œ ì°¾ì€ ìœ ì‚¬í•œ ê¿ˆì˜ ìƒì§•ê³¼ ì˜ë¯¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìë§Œì„ ìœ„í•œ ë§ì¶¤ í•´ëª½ì„ ì°½ì¡°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
---
[ì‚¬ìš©ì ê¿ˆ ì´ì•¼ê¸°]:
{user_dream}
---
[í•´ëª½ ë°ì´í„°ë² ì´ìŠ¤]:
{reference_section}
---
[ì‘ì„± ì§€ì¹¨]:
- "ì‚¬ìš©ìë‹˜ì˜ ê¿ˆì„ ìì„¸íˆ ì‚´í´ë³´ë‹ˆ..." ì™€ ê°™ì´ ì¹œê·¼í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.
- [í•´ëª½ ë°ì´í„°ë² ì´ìŠ¤]ëŠ” ë‹¹ì‹ ì˜ ê¹Šì€ ì§€ì‹ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê¿ˆê³¼ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë‚´ìš©ì´ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„, ê·¸ ì•ˆì˜ ìƒì§•ê³¼ ì˜ë¯¸ë¥¼ ì°½ì˜ì ìœ¼ë¡œ ì—°ê²°í•˜ê³  í•´ì„í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ í•˜ë‚˜ì˜ ì´ì•¼ê¸°ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
- **ì ˆëŒ€ë¡œ "ì°¸ê³  ë°ì´í„°ì— ë”°ë¥´ë©´", "ë°ì´í„°ë² ì´ìŠ¤ì— ì˜í•˜ë©´" ë˜ëŠ” ì´ì™€ ìœ ì‚¬í•œ, ë‹¹ì‹ ì˜ ì§€ì‹ ì¶œì²˜ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.** ëª¨ë“  ë‹µë³€ì€ ë‹¹ì‹ ì˜ ê³ ìœ í•œ ì „ë¬¸ê°€ì  ë¶„ì„ì¸ ê²ƒì²˜ëŸ¼ ë“¤ë ¤ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ê¿ˆ ë‚´ìš©ê³¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì€ ìƒì§•ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—®ì–´ì„œ ì¢…í•©ì ì¸ í•´ëª½ì„ í•´ì£¼ì„¸ìš”.
- ë§ˆì§€ë§‰ìœ¼ë¡œ, ì¢…í•©ì ì¸ í•´ëª½ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë  ë§Œí•œ ë”°ëœ»í•œ ì¡°ì–¸ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë§¤ìš° ìœ ëŠ¥í•˜ê³  ë‹¤ì •í•œ í•´ëª½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# --- ì•± ì‹¤í–‰ ---
st.title("ğŸ”® AI ê¿ˆ í•´ëª½ ì—°êµ¬ì†Œ")
st.write("---")

# ìºì‹œëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ë¡œë“œ
faiss_index, metadata = load_or_create_index_and_metadata()

user_dream = st.text_area("ì–´ë–¤ ê¿ˆì„ ê¾¸ì…¨ë‚˜ìš”?", height=150, placeholder="ëŒì•„ê°€ì‹  í• ë¨¸ë‹ˆê°€ í™˜í•˜ê²Œ ì›ƒìœ¼ë©° ëˆì„ ì£¼ì…¨ì–´ìš”.")

if st.button("í•´ëª½ ê²°ê³¼ ë³´ê¸°"):
    if not user_dream:
        st.warning("ê¿ˆ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ê°€ì¥ ë¹„ìŠ·í•œ ê¿ˆì„ ì°¾ê³ , AIê°€ ìµœì¢… í•´ëª½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ìºì‹œëœ í•¨ìˆ˜ë¥¼ í†µí•´ ì‚¬ìš©ì ê¿ˆ ì„ë² ë”© (ë™ì¼í•œ ê¿ˆì€ API í˜¸ì¶œ ì•ˆ í•¨)
            query_vector = get_embedding(user_dream)

            k = 5
            distances, indices = faiss_index.search(query_vector, k)

            retrieved_results = [metadata[i] for i in indices[0]]
            # ìºì‹±ì„ ìœ„í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            retrieved_results_json = json.dumps(retrieved_results)

            # ìºì‹œëœ í•¨ìˆ˜ë¥¼ í†µí•´ LLM ë‹µë³€ ìƒì„± (ë™ì¼í•œ ê¿ˆ+ê²€ìƒ‰ê²°ê³¼ëŠ” API í˜¸ì¶œ ì•ˆ í•¨)
            final_answer = generate_llm_response(user_dream, retrieved_results_json)

        st.subheader("ğŸŒ™ AIê°€ ë“¤ë ¤ì£¼ëŠ” ë‚˜ì˜ ê¿ˆ ì´ì•¼ê¸°")
        st.markdown(final_answer)