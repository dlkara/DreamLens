import os
import time
import json
import numpy as np
import openai
import streamlit as st
import faiss
from dotenv import load_dotenv

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv(".env")
openai.api_key = os.getenv("OPENAI_API_KEY")

# --- FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„± ---
@st.cache_resource
def load_data_and_build_index():
    os.makedirs("./embeddings", exist_ok=True)
    os.makedirs("./data", exist_ok=True)
    index_path = "./embeddings/dream.index"
    metadata_path = "./data/meta_dream.json"
    original_data_path = "./data/dream.json"

    try:
        with open(original_data_path, 'r', encoding='utf-8') as f:
            total_data = json.load(f)
    except FileNotFoundError:
        st.error(f"'{original_data_path}' íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    if os.path.exists(index_path) and os.path.exists(metadata_path):
        faiss_index = faiss.read_index(index_path)
        with open(metadata_path, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        st.info("ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        documents, metadata = [], []
        for main_cat, sub_cats in total_data.items():
            for sub_cat, dreams in sub_cats.items():
                for dream_item in dreams:
                    dream_text = dream_item.get('ê¿ˆ', '').strip()
                    interp_text = dream_item.get('í•´ëª½', '').strip()
                    if dream_text and interp_text:
                        full_text = f"ê¿ˆ: {dream_text}\ní•´ëª½: {interp_text}"
                        documents.append(full_text)
                        metadata.append({"ëŒ€ë¶„ë¥˜": main_cat, "ì†Œë¶„ë¥˜": sub_cat, "ê¿ˆ": dream_text, "í•´ëª½": interp_text})

        vectors = []
        batch_size = 1000
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            response = openai.embeddings.create(input=batch, model="text-embedding-3-small")
            vectors.extend([r.embedding for r in response.data])
            time.sleep(1)

        vector_array = np.array(vectors).astype("float32")
        faiss_index = faiss.IndexFlatL2(vector_array.shape[1])
        faiss_index.add(vector_array)
        faiss.write_index(faiss_index, index_path)
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    return faiss_index, metadata

# --- ì„ë² ë”© ìƒì„± ---
@st.cache_data(show_spinner=False)
def get_embedding(text, model="text-embedding-3-small"):
    response = openai.embeddings.create(input=[text], model=model)
    return np.array([response.data[0].embedding], dtype='float32')

# --- í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰ ---
@st.cache_data(show_spinner=False)
def search_by_keywords(keywords, _faiss_index, metadata, top_k=5):
    query_text = " ".join(keywords)
    query_vector = get_embedding(query_text)
    distances, indices = _faiss_index.search(query_vector, top_k)
    return [metadata[i] for i in indices[0]]

# --- GPT í•´ëª½ ìƒì„± ---
@st.cache_data(show_spinner=False)
def generate_llm_from_keywords(keywords, reference_data):
    reference_texts = []
    for i, item in enumerate(reference_data):
        dream = item.get("ê¿ˆ", "").strip()
        interp = item.get("í•´ëª½", "").strip()
        reference_texts.append(f"{i+1}. ê¿ˆ: {dream}\n   í•´ëª½: {interp}")

    context = "\n\n".join(reference_texts)
    keyword_text = ", ".join(keywords)

    prompt = f"""
ë‹¹ì‹ ì€ ê¿ˆ í•´ëª½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ìœ ì‚¬í•œ ê¿ˆ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤:

{context}

ì´ ë°ì´í„°ë¥¼ ì°¸ê³ í•´ì„œ, '{keyword_text}'ë¼ëŠ” í‚¤ì›Œë“œë¥¼ ì¡°í•©í•œ ê¿ˆì— ëŒ€í•´ í•´ëª½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ ë„¤ ê°€ì§€ êµ¬ì„±ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”. ê° í•­ëª©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ë¶„ìë¡œ ì‹œì‘í•˜ì„¸ìš”.

[í•´ëª½ì‹œì‘] ...  
[ìš”ì•½ì‹œì‘] ...  

í•´ëª½ì€ ë¶€ë“œëŸ½ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ê¿ˆ í•´ëª½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
    )
    return response.choices[0].message.content

# --- Streamlit UI ---
st.set_page_config("ê¿ˆ ì¡°í•©ê¸°", page_icon="ğŸ’¡")
st.title("ğŸ’¡ ê¿ˆ ì¡°í•©ê¸°")
st.markdown("í‚¤ì›Œë“œ 1~3ê°œë¥¼ ì…ë ¥í•˜ë©´ AIê°€ ê¿ˆì˜ í•´ëª½ì„ ì§€ì–´ì¤˜ìš”.")

faiss_index, metadata = load_data_and_build_index()

user_keywords = st.text_input("í‚¤ì›Œë“œ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: ë¶ˆ, ë±€, ì–´ë¨¸ë‹ˆ")
if st.button("ğŸ”® í•´ëª½ ìƒì„±"):
    keywords = [kw.strip() for kw in user_keywords.split(",") if kw.strip()]
    if not (1 <= len(keywords) <= 3):
        st.warning("1~3ê°œì˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("GPTê°€ í•´ëª½ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            retrieved = search_by_keywords(keywords, faiss_index, metadata)
            gpt_result = generate_llm_from_keywords(keywords, retrieved)

            try:
                _, interp = gpt_result.split("[í•´ëª½ì‹œì‘]", 1)
                interp, summary = interp.split("[ìš”ì•½ì‹œì‘]", 1)
            except Exception:
                interp, summary = gpt_result, ""

            st.subheader("ğŸŒ™ í•´ëª½")
            st.markdown(interp.strip())

            st.subheader("ğŸ§¾ ìš”ì•½")
            st.markdown(summary.strip())
